---
title: "TP3 Krigeage - Challenge 2025-2026"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#1. MUNOZ Enzo 
#2. MUNIER Hugo
#3. GUILBAUD Maxime
```

# Notre prédiction

```{r seed}
set.seed(1234)
library(ggplot2)
library(GGally)
library(sp)
library(DiceKriging)
library(caret)
library(FactoMineR)
library(gstat)
```

## Collecte du jeu de données
```{r}
Observations <- read.csv("defi_observations.csv", header = TRUE)
X<- Observations[,-8]
Y<- Observations[,8]
n<- length(Y)
d<- length(X)

X_new = read.csv("defi_apredire.csv", header = TRUE)
range_Y<- max(Y) - min(Y)
```

## Collecte du jeu de données
```{r}
cat("Observations : ", nrow(Observations), "lignes,", ncol(Observations), "colonnes\n")
cat("Apredire     : ", nrow(X_new),     "lignes,", ncol(X_new),     "colonnes\n")

cat("\nColonnes Observations :\n"); print(colnames(Observations))
cat("\nColonnes Apredire :\n"); print(colnames(X_new))

cat("\nStructure Observations:\n"); str(Observations)
cat("\nStructure Apredire:\n"); str(X_new)

cat("\nSummary Observations:\n"); print(summary(Observations))
cat("\nSummary Apredire:\n"); print(summary(X_new))

cat("\nNA par colonne (Observations):\n"); print(colSums(is.na(Observations)))
cat("\nNA par colonne (Apredire):\n"); print(colSums(is.na(X_new)))
```
 Il n'y a que des variables entre 0 et 1 de moyenne proche de 0.5, ce qui est équivalent a tiré une loi uniforme dans [0,1].

## Visualisation du jeu de données
```{r}
Xcols <- paste0("X", 1:7)

ggpairs(
  Observations,
  columns = which(colnames(Observations) %in% Xcols),
  aes(color = Y, alpha = 0.6),
  upper = list(continuous = "points"),
  lower = list(continuous = "points"),
  diag  = list(continuous = "densityDiag")
) +
  scale_color_viridis_c() +
  theme(
    legend.position = "none",
    axis.text  = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank()
  )

ggplot(Observations, aes(x = X1, y = X2, color = Y)) +
  geom_point(size = 2) +
  scale_color_viridis_c() +
  labs(
    title = "Visualisation brute des observations",
    x = "X1",
    y = "X2",
    color = "Y"
  ) +
  theme_minimal()
```
En examinant cette matrice, on voit clairement que Y augmente lorsque les varaibles X1 et X2 augmentent. En revanche, aucune relation claire n'apparait entre les variables X3 à X7. Les valeurs de Y sont dispersées. On en déduit que X1 et X2 jouent un rôle important dans la structuration de Y. 

```{r}
# Y ~ X1
ggplot(Observations, aes(x = X1, y = Y)) +
  geom_point(alpha = 0.4, size = 1) +
  geom_smooth(se = FALSE) +
  labs(title = "Tendance globale : Y en fonction de X1", x = "X1", y = "Y") +
  theme_minimal()

# Y ~ X2
ggplot(Observations, aes(x = X2, y = Y)) +
  geom_point(alpha = 0.4, size = 1) +
  geom_smooth(se = FALSE) +
  labs(title = "Tendance globale : Y en fonction de X2", x = "X2", y = "Y") +
  theme_minimal()
```
Ces deux graphiques confirment que Y a une tendance globale croissante en fonction de X1 et de X2. Mais cette tendance est plus prononcée pour X2. De plus, on remarque sur les deux graphiques la présence d'une dispersion. Ces deux graphes nous conduisent à faire recours à un krigeage avec une tendance, soit le krigeage universel. 

```{r}
# Création de bandes de X2 (4 groupes)
Observations$X2_bin <- cut(
  Observations$X2,
  breaks = quantile(Observations$X2, probs = seq(0, 1, 0.25)),
  include.lowest = TRUE
)

ggplot(Observations, aes(x = X1, y = Y, color = X2_bin)) +
  geom_point(alpha = 0.25, size = 1) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Y ~ X1 par bandes de X2 (tester additif vs interaction)",
    x = "X1", y = "Y", color = "Bande X2"
  ) +
  theme_minimal()

Observations$X1_bin <- cut(
  Observations$X1,
  breaks = quantile(Observations$X1, probs = seq(0, 1, 0.25)),
  include.lowest = TRUE
)

ggplot(Observations, aes(x = X2, y = Y, color = X1_bin)) +
  geom_point(alpha = 0.25, size = 1) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Y ~ X2 par bandes de X1 (tester additif vs interaction)",
    x = "X2", y = "Y", color = "Bande X1"
  ) +
  theme_minimal()
```
Ces graphiques nous permettent de faire une analyse par bandes. On remarque pour chaque bande de X1, Y augmente avec X2. Les courbes ont aussi des formes très similaires et sont globalement parallèles. Il en est de même pour l'autre graphique. On peut déduire alors une influence plutôt additive de X1 et X2 sur Y, sans interaction forte visible entre ces deux variables.

X1 par bandes de X2 montre homoscélasticité des résidus contrairement à X2 par bande de X1 où la variance des résidus semble augmenté en X2.

```{r}
n_s <- 250
idx <- sample(1:nrow(Observations), n_s)
S <- Observations[idx, c("X1", "X2", "Y")]

# Construire quelques paires aléatoires
m <- 4000
i <- sample(1:n_s, m, replace = TRUE)
j <- sample(1:n_s, m, replace = TRUE)

dx <- S$X1[i] - S$X1[j]
dy <- S$X2[i] - S$X2[j]
dist_ij <- sqrt(dx^2 + dy^2)
dY <- abs(S$Y[i] - S$Y[j])

df_pairs <- data.frame(dist = dist_ij, dY = dY)

ggplot(df_pairs, aes(x = dist, y = dY)) +
  geom_point(alpha = 0.2, size = 1) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Variabilité locale : |ΔY| en fonction de la distance (X1,X2)",
    x = "Distance entre points",
    y = "|ΔY|"
  ) +
  theme_minimal()
```
On a voulu évaluer la dépendance spatiale de Y. Pour cela, on a tracé la différence absolue des valeurs de Y en fonction de la distance entre les points. On voit que les différences de Y augmentent avec la distance. Ainsi, les points proches présentent des valeurs plus similaires que les points éloignés. De plus, on voit que pour des distances très faible la différence absolue n'est pas forcément nulle ce qui suggère une variabilité locale.

## Hypothèses de modélisation
Notre analyse visuelle met en évidence une tendance globale de la variable Y selon les variables X1 et X2. Par conséquent, un krigeage simple ou ordinaire n’est pas adapté. Les effets de X1 et X2 apparaissant principalement additifs. De plus, la variabilité locale observée suggère un phénomène globalement lisse, mais bruité à petite échelle. Le processus n’est donc pas stationnaire, mais on suppose qu'il peut le devenir en enlevant de la tendance. Finalement, l'ensemble de ces arguments nous conduisent à faire du krigeage universel.

Comme le krigeage universel revient à faire une régression puis du krigeage simple ou ordinaire sur les résidus. On va faire la régression dans la suite

## Régression
```{r}
set.seed(1234)
trainIndex <- createDataPartition(Y, p = 0.8, list = FALSE)

Xtrain <- X[trainIndex, ]
Xtest  <- X[-trainIndex, ]
Ytrain <- Y[trainIndex]
Ytest  <- Y[-trainIndex]

train_df1 <- data.frame(Y = Ytrain, Xtrain)
train_df <- data.frame(Y = Ytrain, Xtrain[,c(1,2)])
train_df2 <- data.frame(Y = Ytrain, Xtrain[,c(1,2,4,5)])
test_df1<- data.frame(Y = Ytest, Xtest)
# Test avec des termes d'ordres 2.
rls_s2 <- lm(Y ~ polym(X1, X2, X3, X6, X7, degree = 2, raw = TRUE) +X4 + X5, data = train_df1)
rls<- lm(Y ~ ., data = train_df)
rls1 <- lm(Y ~ ., data = train_df1)
rls2<- lm(Y ~ ., data = train_df2)
rls1.s <- summary(rls)

```

```{r}
rls1.s
```

```{r}
set.seed(1234)

df_test <- data.frame(Xtest)

y_pred <- predict(rls, newdata = df_test)
y_pred1<- predict(rls1, newdata = df_test)
y_pred2<- predict(rls2, newdata = df_test)
y_pred_s2<- predict(rls_s2, newdata = df_test)

# Calcul du RMSE
rmse <- sqrt(mean((Ytest - y_pred)^2))
print(rmse)
rmse1<- sqrt(mean((Ytest - y_pred1))^2)
print(rmse1)
rmse2<- sqrt(mean((Ytest - y_pred2))^2)
print(rmse2)
rmse_s2<- sqrt(mean((Ytest - y_pred_s2))^2)
print(rmse_s2)
```

La meilleure RMSE obtenue par cross validation est celle obtenue en faisant la régression linéaire simple sur l'ensemble des variables avec une RMSE de 0.0812 contre 2.04lorqu'on régresse que sur les 2 premières variables. L'essai avec les termes d'ordres 2 ne donne pas de meilleures résultats et on a un risque d'overfitting. 

Maintenant qu'on a notre modèle de régression linéaire on va faire le krigeage sur les résidus. 

```{r}
res<- Ytest - y_pred1
plot(res)
print(mean(res))
```

Les résidus sont centrés autour de zéro, et la moyenne est nulle donc on peut utiliser du krigeage simple. Le krigeage ordinaire aurait plus d'incertitude spatiale qui est dû à l'estimation de la moyenne du processus. 

Là ça marche pas parce que en gros il faut split 2 fois pour avoir res_train et res_test

```{r}
set.seed(1234)

res_train <- Ytrain - predict(rls1, newdata = train_df1)
res_test  <- Ytest  - predict(rls1, newdata = test_df1)

crossValidationError <- function(famille, theta) {
  
  # On entraîne sur les résidus du TRAIN uniquement
  monModele <- km(formula = ~1, 
                  design = Xtrain, 
                  response = res_train, 
                  covtype = famille, 
                  coef.trend = 0,      # Krigeage Simple (moyenne 0)
                  coef.cov = theta, 
                  coef.var = var(res_train)) 
  
  prediction <- predict(object = monModele, newdata = Xtest, type="SK")  

  # On compare aux résidus réels du TEST
  error = mean((prediction$mean - res_test)^2)
  plot(res_test, type="l", col="red", main="Comparaison terme à terme")
lines(prediction$mean, col="blue", lty=2)
legend("topright", legend=c("Résidus réels", "Prédiction Krigeage"), col=c("red", "blue"), lty=1:2)
  return(error)
}

theta_vec <- rep(0.3, d)

essai <- crossValidationError("matern3_2", theta_vec)
message("erreur =", essai)
```
```{r}
familles <- c("exp", "matern3_2") 
thetas_a_tester <- c(0.2, 0.3, 0.4, 0.5)

resultats <- expand.grid(Famille = familles, Theta = thetas_a_tester)
resultats$RMSE <- NA

for(i in 1:nrow(resultats)) {
  
  f <- as.character(resultats$Famille[i])
  t_val <- resultats$Theta[i]
  
  theta_vec <- rep(t_val, d) #isotrope
  erreur_mse <- crossValidationError(f, theta_vec)
  
  resultats$RMSE[i] <- sqrt(erreur_mse)
}

resultats_tries <- resultats[order(resultats$RMSE), ]
print(resultats_tries)

rmse_reg_seule <- 0.812
meilleur_rmse <- min(resultats$RMSE)

message("\n--- BILAN ---")
message("RMSE Régression seule : ", rmse_reg_seule)
message("Meilleur RMSE avec Krigeage : ", meilleur_rmse)

if(meilleur_rmse < rmse_reg_seule) {
  gain <- (1 - meilleur_rmse/rmse_reg_seule) * 100
  message("Succès ! Le krigeage améliore le modèle de ", round(gain, 2), "%")
} else {
  message("Le krigeage n'améliore pas la régression avec ces paramètres.")
}
```


## Variogramme empirique
```{r}
data_sp <- Observations
coordinates(data_sp) <- ~ X1 + X2

# distance max entre points (dans l'espace X1,X2)
dmax <- spDists(coordinates(data_sp), longlat = FALSE)
dmax <- max(dmax)

# choix standards (robustes)
cutoff_val <- 0.5 * dmax
width_val  <- cutoff_val / 15  # ~15 classes

vg_Y <- variogram(Y ~ 1, data_sp, cutoff = cutoff_val, width = width_val)
plot(vg_Y, main = "Variogramme empirique - Y brut")
```


```{r}
# Ajustement tendance (additive simple)
trend_lm <- lm(Y ~ X1 + X2, data = Observations)
Observations$resid <- residuals(trend_lm)

# repasser en objet spatial (avec la colonne resid)
data_sp2 <- Observations
coordinates(data_sp2) <- ~ X1 + X2

vg_resid <- variogram(resid ~ 1, data_sp2, cutoff = cutoff_val, width = width_val)
plot(vg_resid, main = "Variogramme empirique - Résidus (Y ~ X1 + X2)")
```


```{r}
vg_dir <- variogram(resid ~ 1, data_sp2,
                    alpha = c(0, 45, 90, 135),
                    cutoff = cutoff_val, width = width_val)

plot(vg_dir, main = "Variogrammes directionnels (résidus)")
```

```{r}



```

```{r}



```


































Voici un exemple pour l'import et l'export:

```{r import export données}
# ce qui est donné:

# lecture: Observations contient les X et les Y correspondants
Observations = read.csv("defi_observations.csv", header = TRUE)

# lecture: Apredire ne contient que des X, il faut prédire les Y
Apredire = read.csv("defi_apredire.csv", header = TRUE)

# Votre prédiction; faites mieux, hein ;-)
Y = Apredire$X1 + mean(Observations$Y)
# votre Y prédit ici X1 + la moyenne des Y, c'est très mauvais!

# Votre exportation
# On concatène d'abord les X avec le Y prédit à l'aide de cbind
MonFichierSoumis =  cbind(Apredire, Y)

# puis on exporte
MonNomDeFichier = "DefiGroupeDURAND.csv" # nom de fichier à adapter hein!!!
write.csv(MonFichierSoumis, MonNomDeFichier, row.names = FALSE)

#on vérifie que c'est bien lisible
LectureDeMonFichier = read.csv(MonNomDeFichier, header = TRUE)

#on fait des vérifications élémentaires, bon nombre de lignes, de colonnes, etc.
#on ne doit voir apparaître que des TRUE, sinon ce n'est pas bon!
message(nrow(LectureDeMonFichier) == 100, ": bon nombre de lignes")
message(ncol(LectureDeMonFichier) == 8, ": bon nombre de colonnes")
message(abs(LectureDeMonFichier[37,3]-Apredire[37,3])<1e-6, ": X3 semble ok pour la ligne 37")
message(sum(colnames(LectureDeMonFichier)==c("X1", "X2", "X3", "X4", "X5", "X6", "X7", "Y")) == 8, ": les colonnes sont bien nommées")
```

Voilà, c'est à vous, vous n'avez plus qu'à faire vos prédictions! en remplaçant la ligne `Y = Apredire$X1 + mean(Observations$Y)` bien sûr!

Bon courage!


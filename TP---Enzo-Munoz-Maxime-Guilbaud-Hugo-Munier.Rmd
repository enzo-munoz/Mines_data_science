---
title: "TP - Abres de décision et forêts aléatoires"
author: "Enzo Munoz - Hugo Munier - Maxime Guilbaud"
date: "2025-10-21"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    html_document:
    toc: true
    toc_float: true
    theme: cosmo
header-includes:
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(
  fig.width = 7,        
  fig.height = 4.5,
  out.width = "90%",    
  fig.align = "center", 
  fig.pos = "H"       
)
knitr::opts_chunk$set(
  message = FALSE,   
  warning = FALSE    
)
```

```{r}
# Bibliothèques utilisées

library(GGally)
library(ggplot2)
library(ggcorrplot)
library(corrplot)
library(tidyverse)
library(data.table)
library(janitor)
library(skimr)
library(caret)
library(pROC)
library(randomForest)
library(ranger)
library(rpart)
library(rpart.plot)
```
## Importation des données
```{r}
# Importation des données

data <- read.csv("C:/Users/hugom/OneDrive - Aescra Emlyon Business School/Mines de Saint-Etienne/3A/Science des données/Arbres de décision et forêts aléatoires/TP/7 bank_marketing.csv", 
                 sep=";", header=TRUE)
head(data)
summary(data)
```
Le jeu de données étudié comprend 8 variables de type catégoriel (caractère) et 2 variables numériques (float). L’objectif de notre travail est de développer un modèle prédictif pour la variable class, qui indique si un client souscrit ou non à un dépôt à terme.

## Valeurs manquantes
```{r}
# valeurs manquantes
print("=== Nombre de NA par variable ===")
colSums(is.na(data))
print("=== Nombre de 'unknown' par variable ===")
sapply(data, function(x) sum(x == "unknown", na.rm=TRUE))
```
Le jeu de données ne présente aucune valeur manquante, ce qui nous dispense d’effectuer des étapes de traitement ou d’imputation liées aux données manquantes.

## Transformation des variables
```{r}
# Transformation en facteur
factor_vars <- c("marital", "education", "contact", "poutcome", "default", "housing", "loan", "class")
data[factor_vars] <- lapply(data[factor_vars], as.factor)

# Transformation en numérique
numeric_vars <- c("balance", "age")
data[numeric_vars] <- lapply(data[numeric_vars], as.numeric)

# Vérification
str(data)
summary(data)
```
Afin de pouvoir exploiter correctement l’ensemble des variables dans R, les variables catégorielles ont été converties en facteurs, tandis que les variables quantitatives ont été transformées en numériques.

## Analyse univariée
```{r}
# Analyse univariée
hist(data$age,
     main = "Distribution de l'âge",
     xlab = "Âge",
     col = "skyblue",
     border = "white",
     breaks = 20)

hist(data$balance,
     main = "Distribution du solde (balance)",
     xlab = "Balance (€)",
     col = "orange",
     border = "white",
     breaks = 50,
     xlim = c(-5000, 10000))

boxplot(data$age,
        main = "Boxplot de l'âge",
        col = "skyblue",
        horizontal = TRUE)

boxplot(data$balance,
        main = "Boxplot du solde (balance)",
        col = "orange",
        horizontal = TRUE)


for (var in factor_vars) {
  print(
    ggplot(data, aes_string(x = var, fill = var)) +
      geom_bar() +
      labs(title = paste("Répartition de", var),
           x = var,
           y = "Effectifs") +
      theme_minimal() +
      theme(legend.position = "none")
  )
}
```
L’analyse univariée met en évidence un fort déséquilibre dans les données : la majorité des clients n’ont pas souscrit à un dépôt à terme (class = no), et la variable poutcome est dominée par la modalité failure. Du côté des variables numériques, l’âge est principalement concentré entre 30 et 40 ans, tandis que le solde des comptes présente une distribution asymétrique, avec la majorité des valeurs proches de zéro mais quelques soldes très élevés. Ces constats soulignent à la fois un déséquilibre de classes et la présence de valeurs extrêmes. Les valeurs extrêmes ne posent pas de problèmes car les arbres de décision et les forêts aléatoires n'y sont pas sensibles. En revanche, ce déséquilibre issu de la variable class est à prendre en compte pour la modélisation.

Cette première analyse univariée permet dès le départ de visualiser la distribution des variables et d’en obtenir une première compréhension.

## Analyse bivariée
```{r}
# Analyse bivariée

ggplot(data, aes(x = class, y = age, fill = class)) +
  geom_boxplot() +
  labs(title = "Boxplot de l'âge par classe",
       x = "Classe (Souscription dépôt à terme)",
       y = "Âge") +
  theme_minimal()

ggplot(data, aes(x = age, fill = class)) +
  geom_density(alpha = 0.5) +
  labs(title = "Densité de l'âge selon la classe",
       x = "Âge", y = "Densité") +
  theme_minimal()

ggplot(data, aes(x = class, y = balance, fill = class)) +
  geom_boxplot() +
  labs(title = "Boxplot du solde (balance) par classe",
       x = "Classe (Souscription dépôt à terme)",
       y = "Solde du compte (€)") +
  theme_minimal()

boxplot(balance ~ class, 
        data = data,
        main = "Distribution du solde par classe (zoom)",
        xlab = "Classe (Souscription dépôt à terme)",
        ylab = "Solde (€)",
        col = c("orange", "skyblue"),
        ylim = c(-2000, 10000))

table(data$marital, data$class)
prop.table(table(data$marital, data$class), margin = 1)

# Barplot empilé

ggplot(data, aes(x = marital, fill = class)) +
  geom_bar(position = "fill") +
  labs(title = "Taux de souscription selon marital",
       x = "Situation maritale", y = "Proportion") +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()

table(data$education, data$class)
prop.table(table(data$education, data$class), margin = 1)

ggplot(data, aes(x = education, fill = class)) +
  geom_bar(position = "fill") +
  labs(title = "Taux de souscription selon éducation",
       x = "Niveau d'éducation", y = "Proportion") +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()

for (var in c("housing", "loan", "default")) {
  print(table(data[[var]], data$class))
  print(prop.table(table(data[[var]], data$class), margin = 1))
  
  print(
    ggplot(data, aes_string(x = var, fill = "class")) +
      geom_bar(position = "fill") +
      labs(title = paste("Taux de souscription selon", var),
           x = var, y = "Proportion") +
      scale_y_continuous(labels = scales::percent) +
      theme_minimal()
  )
}

table(data$contact, data$class)
prop.table(table(data$contact, data$class), margin = 1)

ggplot(data, aes(x = contact, fill = class)) +
  geom_bar(position = "fill") +
  labs(title = "Taux de souscription selon type de contact",
       x = "Type de contact", y = "Proportion") +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()

table(data$poutcome, data$class)
prop.table(table(data$poutcome, data$class), margin = 1)

ggplot(data, aes(x = poutcome, fill = class)) +
  geom_bar(position = "fill") +
  labs(title = "Taux de souscription selon résultat campagne précédente",
       x = "Résultat précédent", y = "Proportion") +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()
```
L’analyse bivariée met en évidence plusieurs relations intéressantes avec la variable cible. Nous allons illustré notre porpos avec quelques exemples. Tout d’abord, la densité de l’âge selon la classe montre que les clients plus âgés ont tendance à souscrire davantage à un dépôt à terme, contrairement aux plus jeunes chez qui les refus sont plus fréquents. Ensuite, la variable poutcome illustre une relation claire : les clients dont la campagne précédente s’est soldée par un succès présentent un taux de souscription nettement plus élevé que ceux associés aux modalités failure ou other. Enfin, la variable housing révèle que les clients sans crédit logement sont proportionnellement plus enclins à souscrire, tandis que ceux ayant un prêt immobilier souscrivent beaucoup moins.

## Analyse multidimensionnelle
```{r}
numeric_data <- data[, numeric_vars]

cat_vars <- c("marital", "education", "default", "housing", "loan", "contact", "poutcome","class")
dummies <- dummyVars(~ ., data = data[, cat_vars])
cat_data <- data.frame(predict(dummies, newdata = data))

data_corr <- cbind(numeric_data, cat_data)
cor_matrix <- cor(data_corr, use = "complete.obs")

plot_df <- cbind(data_corr, class = data$class)

vars_to_plot <- c("age", "balance", "marital.married", "marital.divorced","marital.single", "education.tertiary", "housing.yes", "loan.yes", "poutcome.success")

corrplot(cor_matrix, method = "color", type = "upper",
         tl.cex = 0.7,
         title = "Matrice de corrélations")
```
La matrice de corrélations permet d’identifier les relations linéaires entre les variables. On observe que la variable cible class.yes est positivement corrélée avec poutcome.success, confirmant que les clients ayant connu un succès lors de la campagne précédente ont davantage tendance à souscrire. À l’inverse, elle est négativement corrélée avec poutcome.failure et housing.yes, ce qui reflète les tendances mises en évidence dans l’analyse bivariée. Concernant les variables explicatives entre elles, les corrélations restent globalement faibles, ce qui limite le risque de forte multicolinéarité.

En résumé, l’analyse descriptive du jeu de données révèle à la fois un déséquilibre de la variable cible, des relations significatives entre certaines caractéristiques (âge, résultat de campagne, crédit logement) et la probabilité de souscription, ainsi que des corrélations globalement faibles entre variables explicatives, ce qui constitue une base solide pour la phase de modélisation.

## PARTIE I

## Split Apprentissage
```{r}
target_col <- "class"
levels(data[[target_col]])

x_cols <- setdiff(names(data), target_col)

set.seed(1234)
idx <- createDataPartition(data[[target_col]], p = 0.8, list = FALSE)
train <- data[idx, ]
test  <- data[-idx, ]

dim(train)
dim(test)

prop.table(table(train[[target_col]]))
prop.table(table(test[[target_col]]))
```
Dans cette première partie "Split apprentissage", nous avons séparé l’échantillon en deux jeux de données : 80 % pour l’apprentissage et 20 % pour le test. Après cette séparation, on retrouve dans chaque jeu environ 77 % de "no" et 23 % de "yes". Cela montre que le jeu de test reste représentatif de l’ensemble des données et pourra donc servir à évaluer correctement la capacité de généralisation des modèles. Ce point est important, car la classe "yes" étant minoritaire, il faut s’assurer que cette proportion soit bien conservée dans le jeu de test afin d’éviter un biais et d’obtenir des indicateurs de performance fiables.

## Construction de notre arbre de décisions
```{r}
set.seed(1234)
arbre_dd <- rpart(class ~ ., data = train, method = "class", 
                    control = rpart.control(cp = 0.001))

rpart.plot(arbre_dd, type = 2, extra = 106)
```
Nous avons construit un arbre de décision en fixant un paramètre de complexité petit. Ce choix permet à l’algorithme de conserver un grand nombre de divisions. Nous obtenons ainsi à un arbre relativement profond et ramifié, comme le montre la figure ci-dessus. Ce type de modèle peut s’avérer performant sur les données d’apprentissage car il capture de nombreuses nuances, mais il présente également un risque élevé de surapprentissage. En effet, l’arbre devient trop spécifique aux données du train et généralise moins bien sur de nouvelles observations.

Pour pallier ce problème, nous allons par la suite ajuster le paramètre de complexité par un processus d’élagage. La méthode consiste à tracer la courbe d’erreur en fonction de la valeur de cp et à retenir le cp optimal. Cette étape permettra de simplifier l’arbre et d’améliorer sa capacité de généralisation.

## Optimisation de notre arbre de décisions
```{r}
printcp(arbre_dd)  
plotcp(arbre_dd) 

opt_cp <- arbre_dd$cptable[which.min(arbre_dd$cptable[,"xerror"]), "CP"]
cat("\nParamètre cp optimal retenu :", opt_cp, "\n")

opt_cp <- 0.0019
arbre_dd_opti <- prune(arbre_dd, cp = opt_cp)

rpart.plot(arbre_dd_opti, type = 2, extra = 106, under = TRUE)
```
La racine de l’arbre présente une erreur de classification de 22,8 %, ce qui signifie qu’environ un quart des observations sont mal classées au départ. L’analyse de la table de complexité et de la courbe d’erreur montre que l’erreur de validation croisée diminue fortement dès le premier split, puis se stabilise et repart légèrement à la hausse lorsque l’arbre devient plus complexe. Le paramètre cp optimal retenu est donc 0.00165, car il correspond au minimum de l’erreur de validation croisée. Cela permet d’élaguer l’arbre pour conserver un modèle plus simple et robuste, évitant ainsi le surapprentissage tout en maintenant une bonne capacité de généralisation.

## Evaluation sur le jeu de test
```{r}
pred_class <- predict(arbre_dd_opti, newdata = test, type = "class")
pred_prob <- predict(arbre_dd_opti, newdata = test, type = "prob")

conf_matrix <- confusionMatrix(pred_class, test$class, positive = "yes") 
print(conf_matrix)

roc_obj <- roc(response = test$class,
               predictor = pred_prob[, "yes"],
               levels = rev(levels(test$class)))
auc_value <- auc(roc_obj)
cat("AUC =", auc_value, "\n")

plot(roc_obj, col = "blue", lwd = 2, main = "Courbe ROC - Arbre optimal")
```
L’évaluation du modèle optimal sur les données de test montre une accuracy globale de 83 %, significativement supérieure au taux de référence (No Information Rate = 77 %). La matrice de confusion met en évidence une bonne capacité du classifieur à reconnaître les clients n’ayant pas souscrit, avec une spécificité élevée de 91,3 %. En revanche, la sensibilité reste plus limitée (55,1 %), ce qui signifie qu’un peu moins de six clients souscripteurs sur dix sont correctement identifiés. Ce déséquilibre entre précision sur les "no" et rappel sur les "yes" est courant dans des jeux de données où la classe positive est minoritaire. La valeur prédictive positive (65,1 %) et la valeur prédictive négative (87,3 %) confirment cette tendance. Les prédictions "no" sont plus fiables que les prédictions "yes". Le coefficient de Kappa (0,49) traduit un accord modéré entre les prédictions et la réalité.
L’analyse de la courbe ROC indique une bonne capacité de discrimination globale du modèle, supérieure à un classifieur aléatoire (AUC = 0,5), mais encore perfectible pour atteindre des standards élevés supérieurs à 0.8. La courbe ROC présente une montée rapide dès les premiers seuils, ce qui traduit une aptitude du modèle à distinguer efficacement les deux classes dès les probabilités les plus faibles.

En conclusion, le modèle présente une performance satisfaisante avec une bonne précision globale et une capacité discriminante correcte, mais il privilégie la détection des non-souscripteurs au détriment des souscripteurs. Selon les objectifs de l’étude, il pourrait être pertinent de diminuer le seuil de décision (actuellement fixé à 0,5) afin d’augmenter le rappel sur la classe "yes", quitte à introduire davantage de faux positifs. Cette approche permettrait de maximiser la détection des clients susceptibles de souscrire, ce qui constitue souvent l’objectif prioritaire dans un contexte marketing.

## Modèle de régression logistique
```{r}
logit_model <- glm(class ~ ., data = train, family = binomial)

summary(logit_model)

logit_prob <- predict(logit_model, newdata = test, type = "response")

logit_class <- ifelse(logit_prob >= 0.5, "yes", "no")
logit_class <- factor(logit_class, levels = c("no", "yes"))

confusionMatrix(logit_class, test$class, positive = "yes")

roc_logit <- roc(response = test$class,
                 predictor = logit_prob,
                 levels = rev(levels(test$class)))

auc_logit <- auc(roc_logit)
cat("AUC régression logistique =", auc_logit, "\n")

plot(roc_logit, col = "darkgreen", lwd = 2, main = "ROC - Arbre vs Logit")
lines(roc_obj, col = "blue", lwd = 2)
legend("bottomright", legend = c("Logit", "Arbre"),
       col = c("darkgreen", "blue"), lwd = 2)
```
L’arbre optimal atteint une accuracy de 83 % et une AUC de 0,73, avec une bonne capacité à identifier les non-souscripteurs (spécificité de 91 %) mais une sensibilité plus limitée (55 %). La régression logistique présente une accuracy comparable (82,8 %) mais une AUC supérieure (0,82), traduisant une meilleure capacité globale de discrimination entre les classes. On observe toutefois que la sensibilité du modèle logistique (44,8 %) est inférieure à celle de l’arbre, alors que sa spécificité (94 %) est plus élevée. Il privilégie ainsi fortement la détection des "no" au détriment des "yes".

De plus, la régression logistique présente l’avantage d’être plus interprétable d’un point de vue statistique. Les coefficients estimés permettent de quantifier l’effet de chaque variable explicative. Par exemple, un résultat positif à la campagne précédente augmente la probabilité de souscription, tandis qu’un prêt immobilier la réduit. L’arbre de décision est quant à lui plus intuitif visuellement et offre une segmentation opérationnelle claire des clients.

En conclusion, la régression logistique se distingue par sa meilleure calibration probabiliste et son pouvoir discriminant global avec un AUC plus élevé, tandis que l’arbre reste un outil de décision pratique et lisible pour une segmentation marketing.

## PARTIE II

## Schéma de validation et métriques

```{r}
ctrl <- trainControl(
  method = "repeatedcv",
  number = 5, repeats = 2,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  sampling = "down",
  savePredictions = "final"
)
```
Nous avons ensuite mis en place un schéma de validation pour évaluer et comparer les deux modèles de forêts aléatoires. Le jeu d’apprentissage est soumis à une validation croisée à 5 folds, répétée deux fois afin de limiter la variance des estimations. Comme la variable "class" est déséquilibrée, nous appliquons un down sampling à chaque fold afin d’éviter que le modèle n’apprenne à prédire uniquement "no". Les performances sont alors mesurées à l’aide de plusieurs indicateurs : l’AUC/ROC, la sensibilité et la spécificité. Enfin, une évaluation finale est réalisée sur le jeu de test pour estimer la capacité de généralisation de nos deux modèles.

## Arbre de décision illustratif
```{r}
model_tree <- rpart(class ~ ., data = train, method = "class")
rpart.plot(model_tree, type=2, extra=104, main="Arbre de décision")
```
Avant de passer à la modélisation avec les deux forêts aléatoires, nous avons ajouté un arbre de décision à titre de rappel. Celui-ci met en évidence que la variable "poutcome" joue un rôle central dans la prédiction : lorsque poutcome vaut "failure" ou "other", la probabilité de non-souscription atteint 86 %, tandis qu’en cas de "success", 65 % des clients souscrivent à nouveau. Cet arbre illustre bien la logique de segmentation, mais il reste limité puisqu’il ne repose que sur une règle principale et demeure instable. Pour obtenir un modèle plus robuste et généralisable, il est donc nécessaire d’utiliser une forêt aléatoire, issue de la combinaison de nombreux arbres.

## Implémentation A
```{r}
grid_rf <- expand.grid(
  mtry = floor(seq(2, sqrt(length(x_cols))*2, length.out = 6))
)

set.seed(1234)
rf_fit <- train(
  x = train[, x_cols],
  y = train[[target_col]],
  method = "rf",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = grid_rf,
  ntree = 500,
  importance = TRUE
)

print(rf_fit)

plot(rf_fit, main = "Performance ROC en fonction de mtry (Random Forest)")
```
Notre premier modèle repose sur l’implémentation classique du Random Forest. Nous avons testé différentes valeurs de l’hyperparamètre mtry, qui correspond au nombre de variables candidates sélectionnées aléatoirement à chaque division de nœud. L’évaluation a été réalisée selon le schéma de validation décrit précédemment. Les résultats montrent que la performance, mesurée par l’AUC (ROC), diminue lorsque mtry augmente. Le maximum est atteint pour mtry = 2 avec une AUC de 0,796, puis elle baisse progressivement jusqu’à 0,761 pour mtry = 6. En effet un nombre trop important de variables à chaque split réduit la diversité des arbres et favorise le sur-apprentissage, d'pù cette évolution de l'AUC. Le modèle optimal retenu est donc celui avec mtry = 2. Il offre également le meilleur compromis entre sensibilité (0,761) et spécificité (0,721). Ce réglage maximise la capacité discriminante de la forêt aléatoire et améliore la détection des souscripteurs, objectif principal de notre étude.

## Imlplémentation B
```{r}
grid_rg <- expand.grid(
  mtry = floor(seq(2, sqrt(length(x_cols))*2, length.out = 6)),
  splitrule = c("gini", "extratrees"),
  min.node.size = c(1, 5, 10)     
)

set.seed(1234)
rg_fit <- train(
  x = train[, x_cols],
  y = train[[target_col]],
  method = "ranger",
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = grid_rg,
  num.trees = 500,
  importance = "permutation",
  respect.unordered.factors = "partition"
)

print(rg_fit)

plot(rg_fit, main = "Performance ROC selon hyperparamètres (Ranger)")
```
Notre second modèle repose sur l’implémentation Ranger, choisie car elle permet un réglage plus fin des hyperparamètres, même si son coût de calcul est plus élevé que celui du Random Forest. Elle offre notamment la possibilité de contrôler la structure des arbres via différents paramètres : min.node.size (taille minimale des nœuds terminaux, jouant un rôle d’élagage implicite), splitrule (critère de séparation, gini ou extratrees), ainsi que mtry (nombre de variables candidates par split). Ces paramètres ont été optimisés à l’aide de notre schéma de validation. Les résultats montrent que la performance est maximale pour la configuration suivante : mtry = 2, splitrule = extratrees et min.node.size = 1, avec une AUC de 0,801. Cette combinaison favorise la diversité des arbres (faible mtry), des séparations plus aléatoires (extratrees) et une granularité plus fine (nœuds très petits), ce qui améliore la capacité de discrimination du modèle. À l’inverse, lorsque mtry augmente ou que min.node.size devient trop élevé, l’AUC décroît car la forêt perd en diversité et en précision. Le modèle optimal retenu permet ainsi de maximiser la détection des souscripteurs tout en maintenant un bon équilibre entre sensibilité (0,740) et spécificité (0,741).

## Sélection du modèle le plus optimal

```{r}
auc_rf <- max(rf_fit$results$ROC)
auc_rg <- max(rg_fit$results$ROC)

cat("AUC RandomForest :", round(auc_rf, 4), "\n")
cat("AUC Ranger       :", round(auc_rg, 4), "\n")

if (auc_rf >= auc_rg) {
  best_model <- rf_fit
  model_name <- "Random Forest (package randomForest)"
} else {
  best_model <- rg_fit
  model_name <- "Ranger (package ranger)"
}

cat("==> Modèle retenu :", model_name, "\n")
cat("Hyperparamètres optimaux :\n")
print(best_model$bestTune)
```
La comparaison des deux implémentations met en évidence que la forêt aléatoire (randomForest) atteint une AUC maximale de 0,796, tandis que l’implémentation Ranger obtient un score légèrement supérieur, à 0,802. Bien que l’écart soit limité, il souligne l’intérêt de Ranger, qui offre une plus grande flexibilité dans l’ajustement des hyperparamètres. Le modèle optimal retenu correspond à la configuration mtry = 2, splitrule = extratrees et min.node.size = 1. Ce paramétrage favorise une forte diversité entre les arbres et améliore la capacité discriminante de la forêt sur un jeu de données déséquilibré. Ainsi, le modèle Ranger est jugé plus performant et mieux adapté pour la suite de l’analyse, puisqu’il maximise la détection des souscripteurs tout en conservant une bonne généralisation.

## Evaluation sur le jeu de test

```{r}
pred_prob_rf <- predict(rf_fit, newdata = test, type = "prob")[, "yes"]
pred_prob_rg <- predict(rg_fit, newdata = test, type = "prob")[, "yes"]

pred_cls_rf <- factor(ifelse(pred_prob_rf >= 0.5, "yes", "no"), levels = c("no","yes"))
pred_cls_rg <- factor(ifelse(pred_prob_rg >= 0.5, "yes", "no"), levels = c("no","yes"))

cm_rf <- confusionMatrix(pred_cls_rf, test[[target_col]], positive = "yes")
cm_rg <- confusionMatrix(pred_cls_rg, test[[target_col]], positive = "yes")

cat("=== RandomForest ===\n")
print(cm_rf)
cat("\n=== Ranger ===\n")
print(cm_rg)

roc_rf <- pROC::roc(response = test[[target_col]], predictor = pred_prob_rf,
                    levels = c("no","yes"), direction = "<")
roc_rg <- pROC::roc(response = test[[target_col]], predictor = pred_prob_rg,
                    levels = c("no","yes"), direction = "<")

auc_rf <- pROC::auc(roc_rf)
auc_rg <- pROC::auc(roc_rg)

cat("\nAUC RandomForest (test) :", round(auc_rf, 3), "\n")
cat("AUC Ranger (test)       :", round(auc_rg, 3), "\n")

plot(roc_rf, legacy.axes = TRUE, col = "blue",
     main = "Courbes ROC sur le jeu de test - RandomForest vs Ranger")
plot(roc_rg, add = TRUE, col = "red")
legend("bottomright",
       legend = c(paste0("RandomForest AUC=", round(auc_rf,3)),
                  paste0("Ranger AUC=", round(auc_rg,3))),
       col = c("blue", "red"), lty = 1, cex = 0.9)
```
L’évaluation finale sur le jeu de test montre que les deux implémentations de forêts aléatoires obtiennent des performances très proches : une AUC de 0,825 pour RandomForest et de 0,821 pour Ranger. Ce résultat nuance l’analyse précédente, où Ranger apparaissait comme le plus performant en validation croisée. L’écart observé reste faible et non significatif, ce qui confirme la robustesse des deux approches face au problème étudié. Le choix final peut donc s’appuyer sur des critères pratiques : Ranger offre davantage de flexibilité dans le réglage des hyperparamètres, tandis que RandomForest affiche une légère supériorité sur le test. Dans les deux cas, la capacité de discrimination est jugée satisfaisante.

L’analyse des matrices de confusion montre que RandomForest atteint une accuracy de 77,2 %, avec une sensibilité de 0,799 et une spécificité de 0,763. De son côté, Ranger présente une accuracy et une une spécificité inférieures, mais leurs sensibilités sont similaires. Dans les deux cas, la capacité à repérer correctement les clients susceptibles de souscrire reste satisfaisante. Cet aspect est particulièrement important dans notre contexte marketing. Il est préférable de détecter un maximum de souscripteurs potentiels, quitte à inclure par erreur certains clients qui ne s’abonneront finalement pas.

Ces résultats confirment que les deux méthodes sont robustes et adaptées, mais montrent également que le gain de performance de Ranger observé en validation croisée ne se retrouve pas de manière significative sur le jeu de test. Le choix entre les deux implémentations peut donc se fonder sur des critères pratiques. Par exemple, RandomForest offre des performances légèrement supérieures en AUC et en spécificité, tandis que Ranger reste intéressant pour sa flexibilité dans l’optimisation des hyperparamètres.

## Importance des variables
```{r}

# Importance des variables - RandomForest
varImp_rf <- varImp(rf_fit, scale = TRUE)
print(varImp_rf)
plot(varImp_rf, top = 15, main = "Importance des variables - RandomForest")

varImp_rg <- varImp(rg_fit, scale = TRUE)
print(varImp_rg)
plot(varImp_rg, top = 15, main = "Importance des variables - Ranger")
```
L’analyse de l’importance des variables met en évidence des résultats cohérents entre les deux implémentations de forêts aléatoires. Dans les deux cas, la variable poutcome apparaît comme la plus déterminante dans la prédiction de la souscription à un dépôt à terme.

La variable housing arrive en seconde position, suivie de loan, de l’âge et du solde du compte. Ces variables traduisent des facteurs socio-économiques influençant directement la probabilité d’épargne ou de souscription.

Les variables marital, education, contact et default ont une importance plus faible, mais contribuent néanmoins à affiner le modèle en segmentant les profils. La cohérence entre RandomForest et Ranger renforce la robustesse de l’interprétation, bien que l’ordre exact des variables secondaires varie légèrement.

En conclusion, ces résultats montrent que l’historique des interactions marketing (poutcome) et les variables financières liées aux crédits et au patrimoine (housing, loan, balance) constituent les principaux déterminants de la souscription, ce qui est cohérent avec les attentes d’un ciblage marketing bancaire.

## PARTIE III

Le modèle de la forêt d’isolement (Isolation Forest) est un algorithme d’apprentissage non supervisé utilisé principalement pour la détection d’anomalies.

 Principe :

Au lieu de modéliser la distribution des données normales, l’Isolation Forest isole directement les observations. Elle construit de nombreux arbres binaires aléatoires où, à chaque nœud, une caractéristique et une valeur de coupure sont choisies au hasard.
Les points anormaux, étant rares et différents, sont plus faciles à isoler : ils nécessitent moins de divisions pour être séparés du reste des données.

 Idée clé :

Moins une observation nécessite de divisions pour être isolée,
➜ plus elle est susceptible d’être une anomalie.

 Avantages :

Très efficace sur de grands volumes de données.
Ne nécessite pas d’étiquettes (non supervisé).
Gère bien les données de grande dimension.

```{r}
data<- read.csv("KPIs for telecommunication.csv", sep = ";")
```

```{r}

summary(data)
missing_count <- colSums(is.na(data))

cat("\nPourcentage de valeurs manquantes:\n")
print(round(missing_count / nrow(data) * 100, 2))
```


## Gestion des valeurs manquantes (KPI4, KPI6, KPI9)

Pour traiter ces donnees manquantes, nous allons dans un premier temps envisager plusieurs approches :

* Remplacement par la **moyenne** ou la **mediane**.
* Utilisation de la methode des **k plus proches voisins (kNN)**, apres avoir examine la matrice de correlation afin de verifier si les autres KPI influencent les KPI4, KPI6 et KPI9 (qui presentent respectivement 36.70 %, 20.66 % et 27.99 % de valeurs manquantes).
* Suppression eventuelle des variables, si aucune methode d’imputation n’est pertinente.


#1. KNN
```{r}
# Matrice de corrélation sur données complètes

data_complete <- na.omit(data)

# Calcul et visualisation de la corrélation
cor_matrix <- cor(data_complete)


# Visualisation
library(corrplot)

corrplot(cor_matrix, method = "color", type = "upper", 
         addCoef.col = "black", number.cex = 0.7,
         tl.col = "black", tl.srt = 45,
         title = "Matrice de corrélation (données complètes)")

# Corrélations avec les KPI à imputer
cat("\nCorrélations avec KPI4:\n")
print(sort(cor_matrix[, "KPI4"], decreasing = TRUE)[c(1,2,3)])

cat("\nCorrélations avec KPI6:\n")
print(sort(cor_matrix[, "KPI6"], decreasing = TRUE)[c(1,2,3)])

cat("\nCorrélations avec KPI9:\n")
print(sort(cor_matrix[, "KPI9"], decreasing = TRUE)[c(1,2,3)])
```

Apres analyse, aucune des variables (KPI4, KPI6, KPI9) n’est correlee de maniere significative avec les autres. Dans ce cas, l’utilisation du kNN ou d’un modele de regression n’est pas adaptee, car ces methodes reposent justement sur des relations entre variables.

En observant les statistiques descriptives, on constate que la mediane est bien plus representative pour ces indicateurs :

* **KPI4** : min = 80, 1er quartile = 100, max = 100 → la valeur 80 est un outlier ; il est logique d’imputer les valeurs manquantes par 100.
* Le meme raisonnement s’applique a **KPI6** et **KPI9**.

**En conclusion**, avant d’utiliser des methodes plus complexes (comme le kNN, plus couteux en temps de calcul), il est preferable d’analyser les statistiques descriptives. Celles-ci permettent souvent d’opter pour une approche simple, robuste et coherente.

Ainsi, nous choisissons de **ne supprimer aucune observation** et de **remplacer toutes les valeurs manquantes par la mediane de la colonne concernee**.

```{r}
library(dplyr)

data_final <- data %>%
  mutate(across(everything(), ~ifelse(is.na(.x), median(.x, na.rm = TRUE), .x)))
```

```{r}
# Détection des outliers
detect_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR
  upper <- Q3 + 1.5 * IQR
  which(x < lower | x > upper)
}

outliers_list <- lapply(data, detect_outliers)
outliers_count <- sapply(outliers_list, length)

cat("\n### Nombre d'outliers par variable\n")
print(outliers_count)

lignes_outliers <- unique(unlist(outliers_list))

cat("\n**Nombre total de lignes avec au moins un outlier:**", length(lignes_outliers), "\n")

# Stocker les numéros de ligne avec >= 3 outliers
outliers_detected <- c()

for(ligne in lignes_outliers) {
  features_pb <- names(outliers_list)[sapply(outliers_list, function(x) ligne %in% x)]
  
  # Afficher et stocker uniquement si >= 3 outliers
  if(length(features_pb) >= 3) {
    outliers_detected <- c(outliers_detected, ligne)
    valeurs <- sapply(features_pb, function(f) data[ligne, f])
  }
}

cat("\n**Total lignes avec >= 3 outliers:**", length(outliers_detected), "\n")
```
```{r}
cat("\nSummary des outliers par feature:\n")
for(feature in names(outliers_list)) {
  if(length(outliers_list[[feature]]) > 0) {
    cat("\n", feature, ":\n", sep="")
    valeurs_outliers <- data[outliers_list[[feature]], feature]
    print(summary(valeurs_outliers))
  }
}
```

```{r}
# Préparer les données pour le plot
features_with_outliers <- names(outliers_list)[sapply(outliers_list, function(x) length(x) > 0)]

par(mfrow = c(2, ceiling(length(features_with_outliers)/2)))

for(feature in features_with_outliers) {
  valeurs_normales <- data[-outliers_list[[feature]], feature]
  valeurs_outliers <- data[outliers_list[[feature]], feature]
  
  boxplot(list(Normal = valeurs_normales, Outliers = valeurs_outliers),
          main = feature,
          col = c("lightblue", "coral"),
          ylab = "Valeur",
          cex.main = 0.9)
}

par(mfrow = c(1, 1))
```
## Analyse critique de la détection d'outliers par la méthode IQR

En comparant les statistiques des outliers détectés avec la méthode IQR (seuil 1.5 × IQR) et celles des valeurs normales, on observe plusieurs problèmes majeurs :

- **KPI9** : Les outliers détectés ont des statistiques globales très proches des non-outliers, suggérant une sur-détection.
- **KPI8 et KPI6** : Même constat, la méthode IQR capture trop de valeurs qui ne sont pas réellement aberrantes.

Cette méthode statistique classique s'avère **inadaptée à notre jeu de données**. Cependant, elle nous fournit une baseline de **`r length(outliers_detected)` lignes potentiellement aberrantes** qui servira de référence pour évaluer les modèles Isolation Forest.

### Cas particulier : KPI1 = 0

Un problème majeur émerge : environ **330 lignes sur `r nrow(data)` (25%)** sont marquées comme outliers uniquement parce que **KPI1 = 0**. 

> **Question critique** : Doit-on considérer toutes ces lignes comme des anomalies ? **Non, je ne pense pas**. 

Une valeur nulle pour KPI1 peut être légitime selon le contexte métier (absence d'activité, période de maintenance, etc.). Il en va de même pour KPI9. Étant donné qu'on a pas le contexte, on ne peut pas conclure. 

### Critère de sélection du meilleur modèle

Cette méthode pourra quand même nous aider pour juger les modèles Isolation Forest. D'après notre interprétation, le modèle devra : 

1. **Ne pas classifier systématiquement comme anomalie** les lignes où KPI1 = 0. 
Les observations avec KPI1 = 0 ne font pas ressortir en général d'autres potentielles outliers pour les autres feature en partant de ce principe. On va classer les modèles de sorte que le meilleur soit celui qui :
**Maximise le F1-score** en identifiant les lignes avec réellement 3+ features aberrantes simultanément. 

Le meilleur modèle sera donc celui qui fait preuve de **discernement contextuel** et ne se laisse pas piéger par des valeurs nulles fréquentes mais potentiellement normales.


Dans la prochaine partie on va tester l'influence des hyperparamètres sur 4 différents modèles 
```{r}
library(isotree)
library(ggplot2)
library(rsample)
library(dplyr)

# Division train/test
set.seed(123)
splitter <- data_final %>%
  rsample::initial_split(prop = 0.7)
train <- rsample::training(splitter)
test  <- rsample::testing(splitter)


# Définition de 4 modèles différents
configs <- data.frame(
  model = c("Modèle 1", "Modèle 2", "Modèle 3", "Modèle 4"),
  ntrees = c(100, 500, 500, 250),
  sample_size = c(100, 100, 512, 256),
  ndim = c(1, 1, 3, 2)
)

# Stockage des résultats
all_scores <- list()
comparison_results <- data.frame()

for(i in 1:nrow(configs)) {
  # Entraînement du modèle
  model <- isolation.forest(train, 
                           ntrees = configs$ntrees[i],
                           sample_size = configs$sample_size[i],
                           ndim = configs$ndim[i])
  
  # Scores sur test
  scores <- predict(model, test)
  all_scores[[i]] <- data.frame(
    score = scores,
    model = configs$model[i]
  )
  
  # Métriques de comparaison
  comparison_results <- rbind(comparison_results, data.frame(
    model = configs$model[i],
    ntrees = configs$ntrees[i],
    sample_size = configs$sample_size[i],
    ndim = configs$ndim[i],
    variance = var(scores),
    range = max(scores) - min(scores),
    q95 = quantile(scores, 0.95),
    mean = mean(scores),
    sd = sd(scores)
  ))
}

# Combiner tous les scores
all_scores_df <- do.call(rbind, all_scores)

# Graphique comparatif avec facets
ggplot(all_scores_df, aes(x = score)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
  geom_vline(data = all_scores_df %>% 
               group_by(model) %>% 
               summarise(q95 = quantile(score, 0.95)),
             aes(xintercept = q95), 
             color = "red", linetype = "dashed", linewidth = 1) +
  facet_wrap(~model, ncol = 2, scales = "free_y") +
  labs(title = "Distribution des scores d'anomalie - Comparaison des 4 modèles",
       subtitle = "Ligne rouge = 95e percentile",
       x = "Score d'anomalie", 
       y = "Fréquence") +
  theme_minimal()
```


```{r}
# Boxplot comparatif
ggplot(all_scores_df, aes(x = model, y = score, fill = model)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Comparaison des distributions de scores",
       x = "Modèle", 
       y = "Score d'anomalie") +
  theme_minimal() +
  theme(legend.position = "none")

# Density plot superposé
ggplot(all_scores_df, aes(x = score, fill = model)) +
  geom_density(alpha = 0.4) +
  labs(title = "Densité des scores - Comparaison",
       x = "Score d'anomalie", 
       y = "Densité") +
  theme_minimal()
```

## Influence des hyperparamètres sur les modèles Isolation Forest

### Configuration des modèles testés

Nous testons 4 configurations différentes pour comprendre l'impact des hyperparamètres sur la détection d'anomalies :
```{r, echo=TRUE}
configs <- data.frame(
  model = c("Modèle 1", "Modèle 2", "Modèle 3", "Modèle 4"),
  ntrees = c(100, 500, 500, 250),
  sample_size = c(100, 100, 512, 256),
  ndim = c(1, 1, 3, 2)
)
knitr::kable(configs, caption = "Configuration des hyperparamètres testés")
```

### Rôle des hyperparamètres

**1. `ntrees` (Nombre d'arbres)**

- **Principe** : Nombre d'arbres dans la forêt d'isolation
- **Impact** : Plus d'arbres = prédictions plus stables et robustes, mais temps de calcul accru
- **Recommandation** : Généralement entre 100 et 500 arbres

**2. `sample_size` (Taille d'échantillon)**

- **Principe** : Nombre d'observations utilisées pour construire chaque arbre
- **Impact** : 
  - **Petit sample_size** (100-256) : Arbres plus petits, détection fine des anomalies, variance élevée
  - **Grand sample_size** (512+) : Arbres plus profonds, modèle plus stable mais peut manquer des anomalies subtiles
- **Recommandation** : Typiquement 256 observations pour un bon compromis

**3. `ndim` (Nombre de dimensions par split)**

- **Principe** : Nombre de features considérées simultanément pour chaque division de l'arbre
- **Impact** :
  - **ndim = 1** : Splits univariés (une feature à la fois), grande variance, sensible aux outliers individuels
  - **ndim > 1** : Splits multivariés, capture mieux les anomalies contextuelles complexes
- **Recommandation** : ndim = 1 pour détecter des anomalies simples, ndim = 2-3 pour des patterns multivariés

### Analyse comparative des résultats
```{r}
knitr::kable(comparison_results, 
             digits = 4,
             caption = "Métriques de performance des 4 modèles")
```

#### Observations clés

**Modèle 1** (ntrees=100, sample_size=100, ndim=1) :

- **Variance la plus élevée** (`r round(comparison_results$variance[1], 4)`) : attendu avec seulement 100 arbres et ndim=1
- **Q95 le plus haut** (`r round(comparison_results$q95[1], 4)`) : discrimination plus agressive entre normal/anomalie
- **Interprétation** : Modèle le plus "sensible", identifie plus facilement les anomalies mais risque de faux positifs

**Modèle 3** (ntrees=500, sample_size=512, ndim=3) :

- **Range maximal** (`r round(comparison_results$range[3], 4)`) : meilleure séparation globale des scores
- **Q95 plus bas** (`r round(comparison_results$q95[3], 4)`) : seuil d'anomalie plus conservateur
- **Interprétation** : Modèle plus stable et robuste, capture des anomalies multivariées complexes

**Modèle 2 vs Modèle 1** :

- Même configuration (ndim=1, sample_size=100) mais **5× plus d'arbres**
- Variance réduite grâce à l'effet d'ensemble (averaging)
- Q95 inférieur : prédictions plus conservatives

### Quel est le meilleur modèle ?

Sur la base de ces métriques exploratoires :

> **Le Modèle 1 semble le plus prometteur** pour notre tâche, avec son Q95 élevé et sa forte variance indiquant une bonne capacité de discrimination.

**Cependant**, cette conclusion préliminaire doit être **validée** en comparant les prédictions avec nos **`r length(outliers_detected)` outliers détectés par la méthode IQR** (lignes avec ≥3 features aberrantes). Le meilleur modèle sera celui qui :

1. Maximise le **F1-score** sur ces outliers de référence
2. Ne sur-détecte pas les lignes avec KPI1 = 0 comme anomalies systématiques
3. Capture des anomalies **multivariées complexes** plutôt que des seuils univariés

Cette validation fera l'objet de la section suivante.

Pour garantir une évaluation robuste, nous effectuons un **split stratifié** sur la variable `has_multiple_outliers` (indiquant si une ligne contient ≥3 outliers IQR), assurant ainsi que les ensembles train et test maintiennent la **même proportion d'anomalies potentielles** (~`r round(mean(data_final$has_multiple_outliers)*100, 1)`%).


```{r}
# Créer les variables de stratification
data_final$has_multiple_outliers <- 1:nrow(data_final) %in% outliers_detected
data_final$has_kpi1_zero <- data_final$KPI1 == 0

# Combiner les deux critères de stratification
data_final$strata_group <- paste0(
  ifelse(data_final$has_multiple_outliers, "outlier", "normal"),
  "_",
  ifelse(data_final$has_kpi1_zero, "kpi1_0", "kpi1_non0")
)

set.seed(123)
splitter <- data_final %>%
  rsample::initial_split(prop = 0.7, strata = strata_group)

train <- rsample::training(splitter)
test <- rsample::testing(splitter)

# Vérification des proportions
cat("Proportion KPI1=0 dans train:", round(mean(train$has_kpi1_zero)*100, 2), "%\n")
cat("Proportion KPI1=0 dans test:", round(mean(test$has_kpi1_zero)*100, 2), "%\n")

# Recalculer outliers sur test (exclure les colonnes ajoutées)
cols_to_exclude <- c("has_multiple_outliers", "has_kpi1_zero", "strata_group")
outliers_list_test <- lapply(test[, !names(test) %in% cols_to_exclude], detect_outliers)
lignes_outliers_test <- unique(unlist(outliers_list_test))

outliers_detected_test <- c()
for(ligne in lignes_outliers_test) {
  features_pb <- names(outliers_list_test)[sapply(outliers_list_test, function(x) ligne %in% x)]
  if(length(features_pb) >= 3) {
    outliers_detected_test <- c(outliers_detected_test, ligne)
  }
}

cat("Outliers détectés dans test set (>= 3 features):", length(outliers_detected_test), "\n")

# ANALYSE : Combien d'outliers détectés ont KPI1 = 0 ?
outliers_with_kpi1_zero <- sum(test$KPI1[outliers_detected_test] == 0)
cat("Parmi les", length(outliers_detected_test), "outliers détectés:\n")
cat("  -", outliers_with_kpi1_zero, "ont KPI1 = 0 (",
    round(outliers_with_kpi1_zero/length(outliers_detected_test)*100, 1), "%)\n")
cat("  -", length(outliers_detected_test) - outliers_with_kpi1_zero, 
    "ont KPI1 ≠ 0\n\n")

# Grille de paramètres
params_grid <- expand.grid(
  ntrees = c(100, 250, 500),
  sample_size = c(128, 256, 512),
  ndim = c(1, 2, 3)
)

# Fonction de comparaison améliorée
compare_outliers <- function(predicted, detected, test_data) {
  communs <- intersect(predicted, detected)
  
  # Taux de détection
  taux <- length(communs) / length(detected) * 100
  
  # F1-score
  precision <- ifelse(length(predicted) > 0, length(communs) / length(predicted), 0)
  recall <- length(communs) / length(detected)
  f1 <- ifelse(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0)
  
  # NOUVEAU : Analyser KPI1 = 0 dans les prédictions
  kpi1_zero_predicted <- sum(test_data$KPI1[predicted] == 0, na.rm = TRUE)
  prop_kpi1_zero <- ifelse(length(predicted) > 0, kpi1_zero_predicted / length(predicted), 0)
  
  return(list(
    communs = length(communs), 
    taux = taux, 
    f1 = f1,
    kpi1_zero_count = kpi1_zero_predicted,
    kpi1_zero_prop = prop_kpi1_zero * 100
  ))
}

# Boucle d'évaluation
results <- data.frame()

for(i in 1:nrow(params_grid)) {
  model_temp <- isolation.forest(train[, !names(train) %in% cols_to_exclude], 
                                 ntrees = params_grid$ntrees[i],
                                 sample_size = params_grid$sample_size[i],
                                 ndim = params_grid$ndim[i])
  
  scores_test <- predict(model_temp, test[, !names(test) %in% cols_to_exclude])
  seuil <- quantile(scores_test, 0.95)
  lignes_predites <- which(scores_test > seuil)
  
  # Comparaison avec analyse KPI1 = 0
  comp_result <- compare_outliers(lignes_predites, outliers_detected_test, test)
  
  results <- rbind(results, data.frame(
    ntrees = params_grid$ntrees[i],
    sample_size = params_grid$sample_size[i],
    ndim = params_grid$ndim[i],
    communs = comp_result$communs,
    taux_detection = comp_result$taux,
    f1_score = comp_result$f1,
    kpi1_zero_count = comp_result$kpi1_zero_count,
    kpi1_zero_pct = comp_result$kpi1_zero_prop
  ))
}

# Classement des modèles
results <- results[order(-results$f1_score), ]

cat("\n### Top 5 modèles (selon F1-score)\n")
print(head(results, 5))

cat("\n### Bottom 5 modèles\n")
print(tail(results, 5))

cat("Le meilleur modèle prédit", results$kpi1_zero_count[1], 
    "outliers avec KPI1=0 (",
    round(results$kpi1_zero_pct[1], 1), "% de ses prédictions)\n")
```


Les 27 configurations testées présentent une convergence remarquable, avec un écart maximal de 4 outliers communs. L'analyse révèle qu'aucun modèle n'identifie les observations avec KPI1 = 0 comme anomalies de manière systématique, démontrant ainsi leur capacité à reconnaître un cluster comme n'étant pas une véritbale anomalie. Cette robustesse contextuelle, combinée à un taux de concordance significatif avec les outliers IQR multivariés (≥3 features aberrantes), confirme bien l'utilité des forêts d'isolation pour la détection d'anomalies complexes dans notre jeu de données.

Une analyse des hyperparamètres révèle que les configurations les plus performantes partagent trois caractéristiques communes :

1. **ndim = 1** : Les splits univariés (une feature à la fois) surpassent les approches multivariées (ndim = 2-3)
2. **sample_size faible** (128-256) : Des échantillons réduits génèrent des arbres peu profonds, plus sensibles aux valeurs extrêmes
3. **ntrees modéré** (100-250) : Un nombre d'arbres limité maintient une variance élevée, favorisant la discrimination

### Lien entre faible corrélation et performance des splits univariés

L'analyse de corrélation préalable révélait des **corrélations faibles à modérées** entre les KPIs. Cette structure de données explique directement pourquoi les modèles avec **ndim = 1** surpassent ceux avec ndim = 2-3 :

**Principe théorique :**
- **Splits univariés (ndim = 1)** : Détectent les anomalies comme des valeurs extrêmes sur une feature isolée
- **Splits multivariés (ndim > 1)** : Détectent les anomalies contextuelles nécessitant la combinaison de plusieurs features

**Application à nos données :**

Lorsque les variables sont **fortement corrélées**, une observation peut être normale sur chaque feature prise individuellement, mais anormale dans leur combinaison (ex : température et pression atmosphérique). Les splits multivariés excellent dans ce cas.

À l'inverse, avec de **faibles corrélations**, les variables évoluent de manière **indépendante**. Les anomalies se manifestent donc principalement comme :
- Des valeurs extrêmes sur KPI1 seul
- Des valeurs aberrantes sur KPI9 seul
- Etc.

**Conclusion :**
La faible structure de corrélation de notre jeu de données rend les **anomalies univariées** (détectables par ndim = 1) plus fréquentes que les **anomalies contextuelles multivariées** (nécessitant ndim > 1). Cela confirme la cohérence entre :
1. La structure de corrélation des données
2. La performance des hyperparamètres
3. La nature des outliers détectés par la méthode IQR (seuils univariés)

```{r}

# Récupérer les 3 modèles à analyser
best_model_1 <- results[1, ]
best_model_2 <- results[2, ]
worst_model <- results[nrow(results), ]

models_to_analyze <- list(
  list(params = best_model_1, name = "Meilleur modèle"),
  list(params = best_model_2, name = "2ème meilleur modèle"),
  list(params = worst_model, name = "Pire modèle")
)

# Fonction pour analyser un modèle
analyze_model <- function(params, model_name) {
  cat("\n### ", model_name, "\n")
  cat("Paramètres: ntrees =", params$ntrees, ", sample_size =", params$sample_size, 
      ", ndim =", params$ndim, "\n")
  cat("F1-score:", round(params$f1_score, 3), "| Taux détection:", 
      round(params$taux_detection, 2), "%\n\n")
  
  # Entraîner le modèle
  model <- isolation.forest(train[, -ncol(train)],
                           ntrees = params$ntrees,
                           sample_size = params$sample_size,
                           ndim = params$ndim)
  
  scores <- predict(model, test[, -ncol(test)])
  test$anomaly_score <- scores
  
  # Top 5 scores les plus hauts
  top5_high <- test[order(-test$anomaly_score), ][1:5, ]
  cat("#### Top 5 scores d'anomalies les plus élevés\n")
  print(top5_high[, c("anomaly_score")])
  cat("\n")
  
  # Top 5 scores les plus bas
  top5_low <- test[order(test$anomaly_score), ][1:5, ]
  cat("#### Top 5 scores d'anomalies les plus bas\n")
  print(top5_low[, c("anomaly_score")])
  cat("\n")
  
  # Statistiques des scores
  cat("#### Statistiques des scores\n")
  cat("- Moyenne:", round(mean(scores), 4), "\n")
  cat("- Médiane:", round(median(scores), 4), "\n")
  cat("- Écart-type:", round(sd(scores), 4), "\n")
  cat("- Min:", round(min(scores), 4), "| Max:", round(max(scores), 4), "\n")
  cat("- Range:", round(max(scores) - min(scores), 4), "\n\n")
  
  # Vérifier si les top anomalies sont dans outliers_detected
  indices_high <- as.numeric(rownames(top5_high))
  in_outliers_high <- sum(indices_high %in% outliers_detected_test)
  cat("Parmi le top 5, ", in_outliers_high, " sont des outliers détectés (IQR)\n\n")
  
  return(scores)
}

# Analyser les 3 modèles
scores_best1 <- analyze_model(best_model_1, "Meilleur modèle")
scores_best2 <- analyze_model(best_model_2, "2ème meilleur modèle")
scores_worst <- analyze_model(worst_model, "Pire modèle")

# Comparaison graphique (2 plots seulement)
par(mfrow = c(1, 2))

# Plot 1: Distribution des scores
boxplot(list(Meilleur = scores_best1, 
             Deuxieme = scores_best2, 
             Pire = scores_worst),
        col = c("lightgreen", "lightblue", "lightcoral"),
        main = "Distribution des scores d'anomalie",
        ylab = "Score")

# Plot 2: Variance des scores
variances <- c(var(scores_best1), var(scores_best2), var(scores_worst))
barplot(variances, 
        names.arg = c("Meilleur", "2ème", "Pire"),
        col = c("lightgreen", "lightblue", "lightcoral"),
        main = "Variance des scores",
        ylab = "Variance")

par(mfrow = c(1, 1))

# Analyse comparative finale
cat("\n### Analyse comparative\n")
cat("**Variance des scores:**\n")
cat("- Meilleur modèle:", round(var(scores_best1), 6), "\n")
cat("- 2ème modèle:", round(var(scores_best2), 6), "\n")
cat("- Pire modèle:", round(var(scores_worst), 6), "\n\n")

cat("**Interprétation:**\n")
cat("Une variance plus élevée indique une meilleure séparation entre anomalies et données normales.\n")
cat("Les meilleurs modèles devraient avoir un range plus large et mieux identifier les outliers IQR.\n")
```